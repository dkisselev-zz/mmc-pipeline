{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiVJ/52deNYz2OWeO8T70G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkisselev-zz/mmc-pipeline/blob/main/Microbiome_Vector_Graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Microbiome Vector Graph"
      ],
      "metadata": {
        "id": "aOfv7Atswo6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authenticate and Configure"
      ],
      "metadata": {
        "id": "0SYK0dEdwoqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 0: Install Libraries\n",
        "# ==============================================================================\n",
        "!pip install gensim pyvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WHK2i5kgsKDT",
        "outputId": "436516d5-81e9-4ba9-c6d0-84dde4fc25ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pyvis) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, jedi, scipy, pyvis, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 jedi-0.19.2 numpy-1.26.4 pyvis-0.3.2 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "ca2afd796b864b33b727cc3b5837d910"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRu5x2UypzsT",
        "outputId": "7824ab9f-307c-4f48-c13e-87b8af0b21f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Step 1: Import Necessary Libraries\n",
        "# ==============================================================================\n",
        "# We need gensim for Word2Vec, networkx for graph manipulation,\n",
        "# and pyvis for beautiful interactive visualizations.\n",
        "# !pip install pandas gensim networkx pyvis beautifulsoup4 requests\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Optional, Any, Set\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tarfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import userdata\n",
        "from google.colab.data_table import DataTable\n",
        "from google.auth import default\n",
        "import google.generativeai as genai\n",
        "import gspread\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 2: Authenticate and Configure variables\n",
        "# ==============================================================================\n",
        "\n",
        "# Grab variables from the form\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1tBjpV_GoXIjx4_3o73Qml0h-BzFBZTD5bc3QHx1l1_4\" # @param {\"type\":\"string\"}\n",
        "worksheet_name = \"Main Data Sheet\" # @param {\"type\":\"string\"}\n",
        "header_indx = 1 # @param {\"type\":\"integer\"}\n",
        "\n",
        "# Get secrets from Colab environment\n",
        "# Authenticate to access Google Sheet\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"Authentication successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed. Please ensure you are in a Google Colab environment. Error: {e}\")\n",
        "\n",
        "# Configure Gemini API\n",
        "try:\n",
        "    API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not configure Gemini API. Please add GOOGLE_API_KEY to your Colab secrets. Error: {e}\")\n",
        "\n",
        "try:\n",
        "    EMAIL = userdata.get('EMAIL')\n",
        "except (ValueError, FileNotFoundError):\n",
        "    raise ValueError(\"EMAIL not found in Colab secrets. Please add it.\")\n",
        "\n",
        "# Load ICD-11 WHO API Keys\n",
        "try:\n",
        "    ICD11_CLIENT_ID = userdata.get('ICD11_CLIENT_ID')\n",
        "    ICD11_CLIENT_SECRET = userdata.get('ICD11_CLIENT_SECRET')\n",
        "    print(\"ICD-11 API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not configure ICD-11 API. Please add ICD11_CLIENT_ID and ICD11_CLIENT_SECRET to your Colab secrets. Error: {e}\")\n",
        "\n",
        "# Load NCBI API Key if it exists\n",
        "try:\n",
        "    NCBI_API_KEY = userdata.get('NCBI_API_KEY')\n",
        "    print(\"NCBI API Key loaded successfully.\")\n",
        "except Exception:\n",
        "    NCBI_API_KEY = None\n",
        "    print(\"NCBI API Key not found in Colab secrets. Proceeding with lower rate limits.\")\n",
        "\n",
        "# Microbial dictionary\n",
        "microbe_dict = 'microbe_dictionary_hierarchical.json'\n",
        "\n",
        "# Disease dictionary\n",
        "disease_dict = 'disease_dictionary_hierarchical.json'\n",
        "\n",
        "ABSTRACT_DICT_PATH = 'abstract_dictionary.json'\n",
        "\n",
        "print(\"âœ… Authenticated and Configured variables\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o75Vq7IoWRlY",
        "outputId": "962edb51-c553-4cdd-fab2-55d31bab3747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication successful.\n",
            "Gemini API configured successfully.\n",
            "ICD-11 API configured successfully.\n",
            "NCBI API Key loaded successfully.\n",
            "âœ… Authenticated and Configured variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Utility Classes"
      ],
      "metadata": {
        "id": "qG4dI5gKHCfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3.1: Abstracting the Taxonomy Interface\n",
        "# ==============================================================================\n",
        "\n",
        "class TaxonomyProvider(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class defining the standard interface for a taxonomy provider.\n",
        "    This ensures that both the NCBI file-based and ICD-11 API-based data\n",
        "    sources can be used interchangeably by the main application logic.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_node(self, node_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Retrieves all available information for a given node ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_parents(self, node_id: str) -> List[str]:\n",
        "        \"\"\"Retrieves a list of parent IDs for a given node ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_children(self, node_id: str) -> List[str]:\n",
        "        \"\"\"Retrieves a list of child IDs for a given node ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_lineage(self, node_id: str) -> Set[str]:\n",
        "        \"\"\"Retrieves the set of all ancestor IDs for a given node ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_name(self, node_id: str) -> str:\n",
        "        \"\"\"Retrieves the primary name for a given node ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_synonyms(self, node_id: str) -> List[str]:\n",
        "        \"\"\"Retrieves a list of synonyms for a given node ID.\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "LP69m9J1_ZSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3.2: NCBI Taxonmy Provider\n",
        "# ==============================================================================\n",
        "class NCBITaxonomy(TaxonomyProvider):\n",
        "    \"\"\"\n",
        "    A concrete implementation of TaxonomyProvider for the NCBI Taxonomy database.\n",
        "    This class handles downloading, parsing, and providing access to the data\n",
        "    from the taxdump files (names.dmp, nodes.dmp).\n",
        "    \"\"\"\n",
        "    def __init__(self, taxdump_dir: str = '.'):\n",
        "        self.taxdump_dir = taxdump_dir\n",
        "        self.nodes_file = os.path.join(self.taxdump_dir, 'nodes.dmp')\n",
        "        self.names_file = os.path.join(self.taxdump_dir, 'names.dmp')\n",
        "\n",
        "        self._ensure_taxdump_files_exist()\n",
        "\n",
        "        print(\"ğŸ§  Loading and processing NCBI taxonomy data...\")\n",
        "        self._load_data()\n",
        "        print(\"âœ… NCBITaxonomy provider initialized.\")\n",
        "\n",
        "\n",
        "    def _ensure_taxdump_files_exist(self):\n",
        "        \"\"\"Downloads and extracts NCBI taxdump files if they don't exist.\"\"\"\n",
        "        if os.path.exists(self.names_file) and os.path.exists(self.nodes_file):\n",
        "            print(\"âœ… NCBI Taxonomy files already exist. Skipping download.\")\n",
        "            return\n",
        "\n",
        "        url = \"https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\"\n",
        "        gz_filename = \"taxdump.tar.gz\"\n",
        "        print(f\"ğŸŒ Downloading NCBI Taxonomy database from {url}...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(gz_filename, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(\"âœ… Download complete.\")\n",
        "\n",
        "            print(\"ğŸ“¦ Extracting files...\")\n",
        "            with tarfile.open(gz_filename, \"r:gz\") as tar:\n",
        "                tar.extractall(path=self.taxdump_dir)\n",
        "            print(\"âœ… Extraction complete.\")\n",
        "            os.remove(gz_filename)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to download or extract NCBI taxdump: {e}\")\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Parses the .dmp files and populates internal data structures.\"\"\"\n",
        "        # --- Part A: Read nodes.dmp for taxonomic structure ---\n",
        "        nodes_df = pd.read_csv(self.nodes_file, sep='|', header=None, engine='python',\n",
        "                               usecols=[0, 1, 2], names=['tax_id', 'parent_tax_id', 'rank'])\n",
        "        nodes_df = nodes_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "        self.parent_map = nodes_df.set_index('tax_id')['parent_tax_id'].to_dict()\n",
        "        self.rank_map = nodes_df.set_index('tax_id')['rank'].to_dict()\n",
        "\n",
        "        # --- Part B: Read names.dmp to collect all microbe names ---\n",
        "        names_df = pd.read_csv(self.names_file, sep='|', header=None, engine='python',\n",
        "                               usecols=[0, 1, 3], names=['tax_id', 'name_txt', 'name_class'])\n",
        "        names_df = names_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "\n",
        "        # Cache names and synonyms\n",
        "        scientific_names = names_df[names_df['name_class'] == 'scientific name']\n",
        "        self.taxid_to_name = scientific_names.set_index('tax_id')['name_txt'].to_dict()\n",
        "\n",
        "        valid_alias_classes = ['synonym', 'equivalent name', 'acronym', 'genbank acronym', 'common name', 'genbank common name']\n",
        "        self.taxid_to_synonyms = names_df[names_df['name_class'].isin(valid_alias_classes)]\\\n",
        "            .groupby('tax_id')['name_txt'].apply(list).to_dict()\n",
        "\n",
        "        # Build children map by reversing the parent map\n",
        "        self.children_map = defaultdict(list)\n",
        "        for child, parent in self.parent_map.items():\n",
        "            self.children_map[parent].append(child)\n",
        "\n",
        "\n",
        "    # --- Implementation of Abstract Methods ---\n",
        "    def get_node(self, node_id: int) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"tax_id\": node_id,\n",
        "            \"parent_tax_id\": self.get_parents(node_id)[0] if self.get_parents(node_id) else None,\n",
        "            \"name\": self.get_name(node_id),\n",
        "            \"rank\": self.rank_map.get(node_id),\n",
        "            \"synonyms\": self.get_synonyms(node_id)\n",
        "        }\n",
        "\n",
        "    def get_parents(self, node_id: int) -> List[int]:\n",
        "        parent = self.parent_map.get(int(node_id))\n",
        "        return [parent] if parent and parent != node_id else []\n",
        "\n",
        "    def get_children(self, node_id: int) -> List[int]:\n",
        "        return self.children_map.get(int(node_id), [])\n",
        "\n",
        "    def get_lineage(self, node_id: int) -> Set[int]:\n",
        "        lineage = set()\n",
        "        current_id = int(node_id)\n",
        "        while current_id in self.parent_map and current_id != self.parent_map[current_id]:\n",
        "            parent_id = self.parent_map[current_id]\n",
        "            lineage.add(parent_id)\n",
        "            current_id = parent_id\n",
        "        return lineage\n",
        "\n",
        "    def get_name(self, node_id: int) -> str:\n",
        "        return self.taxid_to_name.get(int(node_id))\n",
        "\n",
        "    def get_synonyms(self, node_id: int) -> List[str]:\n",
        "        return self.taxid_to_synonyms.get(int(node_id), [])\n",
        "\n"
      ],
      "metadata": {
        "id": "Xg2UZV5sHKOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3.3: Caching Mechanism\n",
        "# A file-based cache that persists between sessions.\n",
        "# ==============================================================================\n",
        "class JsonFileCache:\n",
        "    \"\"\"A simple file-based JSON cache.\"\"\"\n",
        "    def __init__(self, cache_path='api_cache.json'):\n",
        "        self.cache_path = cache_path\n",
        "        self._cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self):\n",
        "        if os.path.exists(self.cache_path):\n",
        "            try:\n",
        "                with open(self.cache_path, 'r') as f:\n",
        "                    print(f\"ğŸ’¾ Loading API cache from '{self.cache_path}'.\")\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"âš ï¸ Cache file is corrupted. Starting with an empty cache.\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def get(self, key: str):\n",
        "        return self._cache.get(key)\n",
        "\n",
        "    def set(self, key: str, value: Any):\n",
        "        self._cache[key] = value\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.cache_path, 'w') as f:\n",
        "            json.dump(self._cache, f, indent=2)\n",
        "        print(f\"ğŸ’¾ API cache saved to '{self.cache_path}'.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3.4: ICD-11 API Handler\n",
        "# This class handles all direct interaction with the WHO ICD-11 API, including\n",
        "# authentication and caching.\n",
        "# ==============================================================================\n",
        "class ICD11Handler:\n",
        "    \"\"\"Handles authentication and data fetching from the WHO ICD-11 API.\"\"\"\n",
        "    def __init__(self, client_id: str, client_secret: str, cache: JsonFileCache):\n",
        "        self.token_url = \"https://icdaccessmanagement.who.int/connect/token\"\n",
        "        self.base_url = \"https://id.who.int/icd\"\n",
        "        self.client_id = client_id\n",
        "        self.client_secret = client_secret\n",
        "        self.access_token = None\n",
        "        self.token_expires_at = None\n",
        "        self.headers = {'Accept': 'application/json', 'API-Version': 'v2', 'Accept-Language': 'en'}\n",
        "        self.cache = cache\n",
        "        self.request_counter = 0\n",
        "\n",
        "    def _get_access_token(self):\n",
        "        # (Logic reused from the original notebook's builder class)\n",
        "        print(\"\\nğŸ”‘ Requesting new ICD-11 API access token...\")\n",
        "        try:\n",
        "            token_data = {'grant_type': 'client_credentials', 'client_id': self.client_id, 'client_secret': self.client_secret, 'scope': 'icdapi_access'}\n",
        "            response = requests.post(self.token_url, data=token_data)\n",
        "            response.raise_for_status()\n",
        "            token_info = response.json()\n",
        "            self.access_token = token_info['access_token']\n",
        "            self.token_expires_at = time.time() + token_info.get('expires_in', 3600) - 300\n",
        "            self.headers['Authorization'] = f'Bearer {self.access_token}'\n",
        "            print(\"âœ… Successfully obtained access token.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Could not obtain ICD-11 access token: {e}\")\n",
        "\n",
        "    def _ensure_valid_token(self):\n",
        "        if not self.access_token or time.time() >= self.token_expires_at:\n",
        "            self._get_access_token()\n",
        "\n",
        "    def get_entity(self, entity_uri: str, use_cache: bool = True) -> Optional[Dict]:\n",
        "        \"\"\"Central data retrieval method with caching.\"\"\"\n",
        "        entity_id = entity_uri.split('/')[-1]\n",
        "        if use_cache and (cached_data := self.cache.get(entity_id)):\n",
        "            return cached_data\n",
        "\n",
        "        self.request_counter += 1\n",
        "        self._ensure_valid_token()\n",
        "\n",
        "        try:\n",
        "            # We need both the entity URI (for parents/children) and the linearization URI (for the code)\n",
        "            entity_res = requests.get(entity_uri, headers=self.headers)\n",
        "            linearization_url = f\"{self.base_url}/release/11/2025-01/mms/{entity_id}\"\n",
        "            linearization_res = requests.get(linearization_url, headers=self.headers)\n",
        "\n",
        "            if entity_res.status_code == 200:\n",
        "                entity_data = entity_res.json()\n",
        "                entity_data['code'] = linearization_res.json().get('code', 'N/A') if linearization_res.status_code == 200 else 'N/A'\n",
        "                self.cache.set(entity_id, entity_data)\n",
        "                return entity_data\n",
        "            return None\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"âš ï¸ API request failed for entity {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "print(\"âœ… ICD-11 Caching and API Handler classes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPXdYa7AHJ7B",
        "outputId": "2a5cd362-d69f-4c08-e08a-18990d055b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ICD-11 Caching and API Handler classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3.5: ICD-11 Taxonomy Provider\n",
        "# It uses the `ICD11Handler` to fetch data and adapts the responses.\n",
        "# ==============================================================================\n",
        "class ICD11Taxonomy(TaxonomyProvider):\n",
        "    \"\"\"Concrete implementation of TaxonomyProvider for the WHO ICD-11 API.\"\"\"\n",
        "    def __init__(self, client_id: str, client_secret: str, cache_path: str = 'icd11_api_cache.json', num_threads: int = 10):\n",
        "        cache = JsonFileCache(cache_path)\n",
        "        self.handler = ICD11Handler(client_id, client_secret, cache)\n",
        "        self.id_to_title_map = {eid: data.get('title', {}).get('@value', 'Unknown')\n",
        "                                for eid, data in self.handler.cache._cache.items()}\n",
        "        self.num_threads = num_threads  # Store the number of threads\n",
        "        print(f\"âœ… ICD11Taxonomy provider initialized with {num_threads} concurrent threads.\")\n",
        "\n",
        "    def _normalize_list(self, value: Any) -> List[str]:\n",
        "        \"\"\"Ensures the API response for parents/children is always a list.\"\"\"\n",
        "        if not value: return []\n",
        "        if isinstance(value, list): return value\n",
        "        return [value]\n",
        "\n",
        "    def get_node(self, node_uri: str) -> Dict[str, Any]:\n",
        "        return self.handler.get_entity(node_uri)\n",
        "\n",
        "    def get_parents(self, node_uri: str) -> List[str]:\n",
        "        data = self.handler.get_entity(node_uri)\n",
        "        return self._normalize_list(data.get('parent')) if data else []\n",
        "\n",
        "    def get_children(self, node_uri: str) -> List[str]:\n",
        "        data = self.handler.get_entity(node_uri)\n",
        "        return self._normalize_list(data.get('child')) if data else []\n",
        "\n",
        "    def get_name(self, node_uri: str) -> str:\n",
        "        # Use local map for speed if available, otherwise fetch\n",
        "        node_id = node_uri.split('/')[-1]\n",
        "        if name := self.id_to_title_map.get(node_id):\n",
        "            return name\n",
        "        if data := self.handler.get_entity(node_uri):\n",
        "            name = data.get('title', {}).get('@value', 'Unknown')\n",
        "            self.id_to_title_map[node_id] = name\n",
        "            return name\n",
        "        return \"Unknown\"\n",
        "\n",
        "\n",
        "    def get_synonyms(self, node_uri: str) -> List[str]:\n",
        "        data = self.handler.get_entity(node_uri)\n",
        "        aliases = set()\n",
        "        if not data: return []\n",
        "        if 'synonym' in data:\n",
        "            aliases.update(s['label']['@value'] for s in data['synonym'])\n",
        "        if 'fullySpecifiedName' in data:\n",
        "            aliases.add(data['fullySpecifiedName']['@value'])\n",
        "        if 'inclusion' in data:\n",
        "            aliases.update(inc['label']['@value'] for inc in data.get('inclusion', []) if 'label' in inc and '@value' in inc['label'])\n",
        "        return sorted(list(aliases))\n",
        "\n",
        "    def get_lineage(self, node_uri: str) -> Set[str]:\n",
        "        # Implements a graph traversal to find all ancestors\n",
        "        queue = [node_uri]\n",
        "        visited = set()\n",
        "        lineage = set()\n",
        "        while queue:\n",
        "            current_uri = queue.pop(0)\n",
        "            if current_uri in visited: continue\n",
        "            visited.add(current_uri)\n",
        "            parents = self.get_parents(current_uri)\n",
        "            for parent_uri in parents:\n",
        "                lineage.add(parent_uri)\n",
        "                queue.append(parent_uri)\n",
        "        return lineage\n",
        "\n",
        "    def build_disease_dictionary(self, root_chapters: Dict[str, str], disease_dict_path: str):\n",
        "        \"\"\"\n",
        "        Builds the full disease dictionary by traversing the ICD-11 graph using a\n",
        "        concurrent, multi-threaded approach to fetch API data.\n",
        "        \"\"\"\n",
        "        print(f\"\\nğŸŒ² Starting concurrent graph traversal from {len(root_chapters)} root chapters...\")\n",
        "        self.handler.request_counter = 0\n",
        "\n",
        "        queue = [f\"{self.handler.base_url}/entity/{root_id}\" for root_id in root_chapters.values()]\n",
        "        visited = set(queue) # Pre-populate visited set to avoid duplicate processing\n",
        "\n",
        "        # Use a ThreadPoolExecutor to manage a pool of worker threads\n",
        "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
        "            while queue:\n",
        "                # Define a batch size, a multiple of the thread count is often efficient\n",
        "                batch_size = self.num_threads * 5\n",
        "                # Take the first `batch_size` items from the queue for processing\n",
        "                current_batch = [queue.pop(0) for _ in range(min(batch_size, len(queue)))]\n",
        "\n",
        "                if not current_batch:\n",
        "                    continue\n",
        "\n",
        "                # Concurrently fetch data for the entire batch. The executor's `map` function\n",
        "                # applies `self.handler.get_entity` to each URI in the batch across multiple threads.\n",
        "                # The list() call forces the execution and waits for all threads to complete.\n",
        "                results = list(executor.map(self.handler.get_entity, current_batch))\n",
        "\n",
        "                # Process the results synchronously to find the next set of children\n",
        "                for entity_data in results:\n",
        "                    if entity_data:\n",
        "                        # Extract child URIs from the fetched data\n",
        "                        children = self._normalize_list(entity_data.get('child'))\n",
        "                        for child_uri in children:\n",
        "                            # Add new, unvisited children to the end of the queue\n",
        "                            if child_uri not in visited:\n",
        "                                visited.add(child_uri)\n",
        "                                queue.append(child_uri)\n",
        "\n",
        "                print(f\"\\r   - API Requests: {self.handler.request_counter}, Entities Processed: {len(visited)}, Queue Size: {len(queue)}  \", end='', flush=True)\n",
        "\n",
        "        print(f\"\\n\\nâœ… Traversal complete. Total unique entities processed: {len(visited)}.\")\n",
        "        self.handler.cache.save()\n",
        "\n",
        "        # The final dictionary processing remains the same\n",
        "        print(\"ğŸ› ï¸  Processing cached data into final dictionary...\")\n",
        "        final_dict = {}\n",
        "        cached_entities = self.handler.cache._cache\n",
        "        for entity_id, data in cached_entities.items():\n",
        "            canonical_name = data.get('title', {}).get('@value')\n",
        "            if not canonical_name: continue\n",
        "            parent_uris = self._normalize_list(data.get('parent'))\n",
        "            parent_id = parent_uris[0].split('/')[-1] if parent_uris else None\n",
        "            parent_data = cached_entities.get(parent_id) if parent_id else {}\n",
        "\n",
        "            if not parent_data:\n",
        "            # If no data is found, assign default \"root\" values\n",
        "              parent_name = \"ICD-11 Root\"\n",
        "              parent_code = \"N/A\"\n",
        "            else:\n",
        "              # If data exists, proceed with the safe extraction\n",
        "              parent_name = parent_data.get('title', {}).get('@value', \"ICD-11 Root\")\n",
        "              parent_code = parent_data.get('code', 'N/A')\n",
        "            final_dict[canonical_name] = {\n",
        "                \"icd11_code\": data.get('code', 'N/A'),\n",
        "                \"parent_name\": parent_name,\n",
        "                \"parent_code\": parent_code,\n",
        "                \"aliases\": self.get_synonyms(data.get('@id'))\n",
        "            }\n",
        "        with open(disease_dict_path, 'w') as f:\n",
        "            json.dump(final_dict, f, indent=2)\n",
        "        print(f\"âœ… Final dictionary with {len(final_dict)} entries saved to '{disease_dict_path}'.\")\n",
        "        return final_dict\n",
        "\n",
        "    print(\"âœ… ICD11Taxonomy provider class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vENJLEoHK5bx",
        "outputId": "50b08ac2-8292-4785-aa98-9add7f35eb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ICD11Taxonomy provider class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3.6: NCBI Publication Metadata fetcher\n",
        "# ==============================================================================\n",
        "class PublicationFetcher:\n",
        "    \"\"\"Handles the fetching of publication abstracts from NCBI.\"\"\"\n",
        "    def __init__(self, email: str, api_key: Optional[str] = None):\n",
        "        self.base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "        self.api_key = api_key\n",
        "        self.email = email\n",
        "        print(\"âœ… PublicationFetcher initialized.\")\n",
        "\n",
        "    def _make_ncbi_request(self, base_url, params=None, data=None, retries=3, delay=2):\n",
        "        \"\"\"Makes a request to the NCBI API with retries and backoff.\"\"\"\n",
        "        # Add a small delay to respect NCBI API rate limits\n",
        "        request_delay = 0.1 if self.api_key else 0.4\n",
        "        time.sleep(request_delay)\n",
        "\n",
        "        # Add API key and email to every request for tracking\n",
        "        if self.api_key:\n",
        "            if params: params['api_key'] = self.api_key\n",
        "            if data: data['api_key'] = self.api_key\n",
        "        if params: params['email'] = self.email\n",
        "        if data: data['email'] = self.email\n",
        "\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                if data:\n",
        "                    response = requests.post(base_url, data=data, timeout=45)\n",
        "                else:\n",
        "                    response = requests.get(base_url, params=params, timeout=45)\n",
        "                response.raise_for_status()\n",
        "                return response.content\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"  > WARNING: Request failed on attempt {attempt + 1}/{retries}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    wait_time = delay * (2 ** attempt)\n",
        "                    print(f\"  > Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "        return None\n",
        "\n",
        "    def get_abstract_by_doi(self, doi: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Fetches a publication's abstract from PubMed using its DOI.\n",
        "\n",
        "        Args:\n",
        "            doi: The Digital Object Identifier of the article.\n",
        "\n",
        "        Returns:\n",
        "            The abstract text as a string, or None if not found.\n",
        "        \"\"\"\n",
        "        if not isinstance(doi, str) or not doi:\n",
        "            return None\n",
        "\n",
        "        # 1. Use esearch to find the PubMed ID (PMID) for the DOI\n",
        "        search_params = {'db': 'pubmed', 'term': f'\"{doi}\"[aid]', 'retmode': 'xml'}\n",
        "        search_response = self._make_ncbi_request(f\"{self.base_url}esearch.fcgi\", params=search_params)\n",
        "        if not search_response:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            pmid_root = ET.fromstring(search_response)\n",
        "            pmid = pmid_root.findtext('.//Id')\n",
        "            if not pmid:\n",
        "                return None\n",
        "        except ET.ParseError:\n",
        "            return None\n",
        "\n",
        "        # 2. Use efetch to get article details with the PMID\n",
        "        fetch_params = {'db': 'pubmed', 'id': pmid, 'retmode': 'xml'}\n",
        "        fetch_response = self._make_ncbi_request(f\"{self.base_url}efetch.fcgi\", params=fetch_params)\n",
        "        if not fetch_response:\n",
        "            return None\n",
        "\n",
        "        # 3. Parse the XML to find the abstract text\n",
        "        try:\n",
        "            article_root = ET.fromstring(fetch_response)\n",
        "            abstract_text_elements = article_root.findall('.//Abstract/AbstractText')\n",
        "            if abstract_text_elements:\n",
        "                # Join text from all AbstractText elements, handling structured abstracts\n",
        "                full_abstract = \" \".join(ET.tostring(elem, method='text', encoding='unicode').strip() for elem in abstract_text_elements)\n",
        "                return full_abstract.strip()\n",
        "            else:\n",
        "                return None # Abstract not found\n",
        "        except ET.ParseError:\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "FrBEe_Q1vpHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Load Data"
      ],
      "metadata": {
        "id": "9CtcvY_OwvEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 4.1: Build Microbial Dictionary form NCBI Tax data\n",
        "# ==============================================================================\n",
        "def is_alias_valid_generalized(alias):\n",
        "    # (Re-using the validation function from the original notebook)\n",
        "    alias_lower = alias.lower()\n",
        "\n",
        "    # Rule 1: Reject if it looks like a strain/culture collection code (e.g., ATCC, DSM, NCTC)\n",
        "    # This also catches things like 'strain ABC' or 'isolate 123'.\n",
        "    if re.search(r'\\b(atcc|nrcc|dsm|nctc|ukmcc|ccug|cip|jcm|lmg|strain|isolate)\\b', alias_lower):\n",
        "        return False\n",
        "\n",
        "    # Rule 2: Reject if it contains a year (likely a citation)\n",
        "    if re.search(r'\\b(18|19|20)\\d{2}\\b', alias_lower):\n",
        "        return False\n",
        "\n",
        "    # Rule 3: Reject if it's a generic placeholder\n",
        "    # if alias_lower.startswith(('bacterium ', 'unidentified ', 'unclassified ', 'endosymbiont of')):\n",
        "        # return False\n",
        "    if any(placeholder in alias_lower for placeholder in ['bacterium ', 'unidentified ', 'unclassified ', 'endosymbiont of']):\n",
        "        return False\n",
        "\n",
        "    # Rule 4: Reject if it ends with a sequence of letters and numbers that looks like a code\n",
        "    if re.search(r'\\s[A-Z0-9\\-_]{5,}$', alias):\n",
        "         return False\n",
        "\n",
        "    # Rule 5: Reject if the name is just a short code\n",
        "    if len(alias) < 4 and not '.' in alias:\n",
        "        return False\n",
        "\n",
        "    # Rule 6: Reject if it contains certain keywords that indicate it's not a standard name\n",
        "    if any(keyword in alias_lower for keyword in ['subgroup', 'serovar', 'genomosp.', ' genomovar']):\n",
        "      return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def build_microbe_dictionary_from_provider(provider: NCBITaxonomy, microbe_dict_path: str):\n",
        "    \"\"\"\n",
        "    Builds the microbe dictionary using the NCBITaxonomy provider.\n",
        "    This function validates that our refactoring works as intended.\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ”¬ Building hierarchical microbe dictionary using the new NCBITaxonomy provider...\")\n",
        "    final_dict = {}\n",
        "\n",
        "    def is_bacterial(tax_id, p_map):\n",
        "      curr_id = tax_id\n",
        "      for _ in range(30):\n",
        "          if curr_id == 2: return True\n",
        "          if curr_id == 1 or curr_id not in p_map: break\n",
        "          curr_id = p_map[curr_id]\n",
        "      return False\n",
        "\n",
        "    target_tax_ids = {tid for tid, rank in provider.rank_map.items() if rank in ['genus', 'species'] and is_bacterial(tid, provider.parent_map)}\n",
        "    print(f\"   - Identified {len(target_tax_ids)} bacterial genus/species tax IDs.\")\n",
        "\n",
        "    for tax_id in target_tax_ids:\n",
        "        canonical_name = provider.get_name(tax_id)\n",
        "        if not canonical_name: continue\n",
        "\n",
        "        aliases = set(provider.get_synonyms(tax_id))\n",
        "        rank = provider.rank_map.get(tax_id)\n",
        "\n",
        "        if rank == 'species':\n",
        "            parts = canonical_name.split()\n",
        "            if len(parts) >= 2: aliases.add(f\"{parts[0][0]}. {parts[1]}\")\n",
        "\n",
        "        filtered_aliases = {alias for alias in aliases if is_alias_valid_generalized(alias)}\n",
        "        if not filtered_aliases: continue\n",
        "\n",
        "        genus_name = None\n",
        "        if rank == 'species':\n",
        "            parent_id = provider.get_parents(tax_id)[0] if provider.get_parents(tax_id) else None\n",
        "            if parent_id and provider.rank_map.get(parent_id) == 'genus':\n",
        "                genus_name = provider.get_name(parent_id)\n",
        "        elif rank == 'genus':\n",
        "            genus_name = canonical_name\n",
        "\n",
        "        final_dict[canonical_name] = {\n",
        "            \"rank\": rank, \"genus\": genus_name, \"aliases\": sorted(list(filtered_aliases))\n",
        "        }\n",
        "\n",
        "    print(f\"   - Final dictionary created with {len(final_dict)} canonical entries.\")\n",
        "    with open(microbe_dict_path, \"w\") as f:\n",
        "        json.dump(final_dict, f, indent=2)\n",
        "    print(f\"\\nâœ… Hierarchical microbe dictionary saved to '{microbe_dict_path}'.\")\n",
        "    return final_dict\n",
        "\n",
        "# --- Execute and Validate ---\n",
        "ncbi_provider = NCBITaxonomy()\n",
        "microbe_dictionary = build_microbe_dictionary_from_provider(ncbi_provider, microbe_dict)\n",
        "\n",
        "# Display a sample entry to validate the output\n",
        "print(\"\\n--- Sample Entries (Validation) ---\")\n",
        "sample_key = \"Escherichia coli\"\n",
        "if sample_key in microbe_dictionary:\n",
        "    print(json.dumps({sample_key: microbe_dictionary[sample_key]}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v27R2wyHrU4",
        "outputId": "9e1b59c1-104d-4e7a-99ee-50830dc1a3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… NCBI Taxonomy files already exist. Skipping download.\n",
            "ğŸ§  Loading and processing NCBI taxonomy data...\n",
            "âœ… NCBITaxonomy provider initialized.\n",
            "\n",
            "ğŸ”¬ Building hierarchical microbe dictionary using the new NCBITaxonomy provider...\n",
            "   - Identified 531722 bacterial genus/species tax IDs.\n",
            "   - Final dictionary created with 512540 canonical entries.\n",
            "\n",
            "âœ… Hierarchical microbe dictionary saved to 'microbe_dictionary_hierarchical.json'.\n",
            "\n",
            "--- Sample Entries (Validation) ---\n",
            "{\n",
            "  \"Escherichia coli\": {\n",
            "    \"rank\": \"species\",\n",
            "    \"genus\": \"Escherichia\",\n",
            "    \"aliases\": [\n",
            "      \"Bacillus coli\",\n",
            "      \"E. coli\",\n",
            "      \"Enterococcus coli\",\n",
            "      \"Escherichia/Shigella coli\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 4.2: Budiling ICD-11 dictionaly\n",
        "# ==============================================================================\n",
        "\n",
        "try:\n",
        "    # Top-level ICD-11 Chapter IDs (unchanged)\n",
        "    root_chapters = {\n",
        "        \"Certain infectious or parasitic diseases\": \"1435254666\",\n",
        "        \"Neoplasms\": \"1630407678\",\n",
        "        \"Diseases of the blood or blood-forming organs\": \"1766440644\",\n",
        "        \"Diseases of the immune system\": \"1954798891\",\n",
        "        \"Endocrine, nutritional or metabolic diseases\": \"21500692\",\n",
        "        \"Mental, behavioural or neurodevelopmental disorders\": \"334423054\",\n",
        "        \"Sleep-wake disorders\": \"274880002\",\n",
        "        \"Diseases of the nervous system\": \"1296093776\",\n",
        "        \"Diseases of the visual system\": \"868865918\",\n",
        "        \"Diseases of the ear or mastoid process\": \"1218729044\",\n",
        "        \"Diseases of the circulatory system\": \"426429380\",\n",
        "        \"Diseases of the respiratory system\": \"197934298\",\n",
        "        \"Diseases of the digestive system\": \"1256772020\",\n",
        "        \"Diseases of the skin\": \"1639304259\",\n",
        "        \"Diseases of the musculoskeletal system or connective tissue\": \"1473673350\",\n",
        "        \"Diseases of the genitourinary system\": \"30659757\",\n",
        "    }\n",
        "\n",
        "    # --- Execute the build process using the new provider ---\n",
        "    icd_provider = ICD11Taxonomy(ICD11_CLIENT_ID, ICD11_CLIENT_SECRET, cache_path='icd11_api_cache.json', num_threads=4)\n",
        "\n",
        "    disease_dictionary = icd_provider.build_disease_dictionary(root_chapters, disease_dict)\n",
        "\n",
        "    # --- Display a sample ---\n",
        "    print(\"\\n--- Sample Entries ---\")\n",
        "    if \"Multiple sclerosis\" in disease_dictionary:\n",
        "        print(json.dumps({\"Multiple sclerosis\": disease_dictionary[\"Multiple sclerosis\"]}, indent=2))\n",
        "    if \"Crohn disease\" in disease_dictionary:\n",
        "        print(json.dumps({\"Crohn disease\": disease_dictionary[\"Crohn disease\"]}, indent=2))\n",
        "\n",
        "except NameError:\n",
        "    print(\"âŒ ERROR: ICD11_CLIENT_ID or ICD11_CLIENT_SECRET not defined.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZt3ORs3LQ1Y",
        "outputId": "f035477e-cbc3-4d02-c0a2-510ef05326cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Loading API cache from 'icd11_api_cache.json'.\n",
            "âœ… ICD11Taxonomy provider initialized with 4 concurrent threads.\n",
            "\n",
            "ğŸŒ² Starting concurrent graph traversal from 16 root chapters...\n",
            "   - API Requests: 0, Entities Processed: 37555, Queue Size: 0  \n",
            "\n",
            "âœ… Traversal complete. Total unique entities processed: 37555.\n",
            "ğŸ’¾ API cache saved to 'icd11_api_cache.json'.\n",
            "ğŸ› ï¸  Processing cached data into final dictionary...\n",
            "âœ… Final dictionary with 37554 entries saved to 'disease_dictionary_hierarchical.json'.\n",
            "\n",
            "--- Sample Entries ---\n",
            "{\n",
            "  \"Multiple sclerosis\": {\n",
            "    \"icd11_code\": \"8A40\",\n",
            "    \"parent_name\": \"Multiple sclerosis or other white matter disorders\",\n",
            "    \"parent_code\": \"\",\n",
            "    \"aliases\": [\n",
            "      \"MS - [multiple sclerosis]\",\n",
            "      \"Multiple sclerosis generalised\",\n",
            "      \"Multiple sclerosis of brain stem\",\n",
            "      \"Multiple sclerosis of cord\",\n",
            "      \"cerebrospinal sclerosis\",\n",
            "      \"disseminated brain sclerosis\",\n",
            "      \"disseminated cerebrospinal sclerosis\",\n",
            "      \"disseminated multiple sclerosis\",\n",
            "      \"disseminated nervous system myelosclerosis\",\n",
            "      \"disseminated sclerosis\",\n",
            "      \"disseminated spinal sclerosis\",\n",
            "      \"generalised multiple sclerosis\",\n",
            "      \"insular brain sclerosis\",\n",
            "      \"insular sclerosis\",\n",
            "      \"miliary brain sclerosis\",\n",
            "      \"multiple ascending sclerosis\",\n",
            "      \"multiple brain sclerosis\",\n",
            "      \"multiple cerebrospinal sclerosis\",\n",
            "      \"multiple combined sclerosis\",\n",
            "      \"multiple combined sclerosis of spinal cord\",\n",
            "      \"multiple sclerosis of the brain stem\",\n",
            "      \"multiple sclerosis of the spinal cord\",\n",
            "      \"plaque sclerosis\",\n",
            "      \"scl\\u00e9rose en plaques\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "{\n",
            "  \"Crohn disease\": {\n",
            "    \"icd11_code\": \"DD70\",\n",
            "    \"parent_name\": \"Inflammatory bowel diseases\",\n",
            "    \"parent_code\": \"\",\n",
            "    \"aliases\": [\n",
            "      \"CD - [Crohn's disease]\",\n",
            "      \"Cobble-stone appearance of intestine\",\n",
            "      \"Crohn disease NOS\",\n",
            "      \"Crohn's disease\",\n",
            "      \"Crohn's regional enteritis\",\n",
            "      \"Crohns\",\n",
            "      \"Granulomatous enteritis\",\n",
            "      \"Intestinal ulcer and erosion due to Crohn disease\",\n",
            "      \"Regional enteritis\",\n",
            "      \"regional enteritis of bowel\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 4.3: Define Function to Build Abstract Dictionary\n",
        "# ==============================================================================\n",
        "def build_abstract_dictionary(fetcher, df, output_path):\n",
        "    \"\"\"Iterates through DOIs in the dataframe, fetches abstracts, and saves to a JSON file.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"âœ… Abstract dictionary '{output_path}' already exists. Skipping build process.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nğŸ“š Building Abstract Dictionary... This may take a while.\")\n",
        "    abstract_dict = {}\n",
        "    total_dois = len(df['DOI'].unique())\n",
        "\n",
        "    for i, doi in enumerate(df['DOI'].unique()):\n",
        "        if not doi: continue\n",
        "\n",
        "        print(f\"\\r   - Processing DOI {i+1}/{total_dois}: {doi}\", end=\"\")\n",
        "        abstract = fetcher.get_abstract_by_doi(doi)\n",
        "        abstract_dict[doi] = abstract if abstract else \"\"\n",
        "\n",
        "    print(\"\\nâœ… Abstract fetching complete.\")\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(abstract_dict, f, indent=2)\n",
        "    print(f\"ğŸ’¾ Abstract dictionary saved to '{output_path}'.\")\n"
      ],
      "metadata": {
        "id": "G6BWchCFwQJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 5: Loading Data\n",
        "# Step 5.1 Download stopwords from NLTK\n",
        "# ==============================================================================\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLF0CtZoWxHb",
        "outputId": "6310bcaa-abc6-4820-c991-a6d96c35b82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 5.2: Load Data from Google Sheet\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Loading data from Google Sheet ---\")\n",
        "try:\n",
        "    spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "    worksheet = spreadsheet.worksheet(worksheet_name)\n",
        "    all_values = worksheet.get_all_values()\n",
        "    header = all_values[header_indx]\n",
        "\n",
        "    # Make column names unique if there are duplicates\n",
        "    cols = pd.Series(header)\n",
        "    for dup in cols[cols.duplicated()].unique():\n",
        "        cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
        "    header = list(cols)\n",
        "\n",
        "    data_rows = all_values[header_indx+1:]\n",
        "    input_df = pd.DataFrame(data_rows, columns=header)\n",
        "    input_df.reset_index(inplace=True)\n",
        "    input_df.rename(columns={'index': 'row_index'}, inplace=True)\n",
        "\n",
        "    # Select the first 'StudyTitle' column by its index if there are multiple\n",
        "    study_title_cols = [col for col in input_df.columns if 'StudyTitle' in col]\n",
        "    if study_title_cols:\n",
        "        input_df['StudyTitle'] = input_df[study_title_cols[0]]\n",
        "        # Drop other StudyTitle columns if they exist, keeping only the first\n",
        "        other_study_title_cols = study_title_cols[1:]\n",
        "        if other_study_title_cols:\n",
        "            input_df.drop(columns=other_study_title_cols, inplace=True)\n",
        "\n",
        "\n",
        "    if 'Processed' not in input_df.columns:\n",
        "        input_df['Processed'] = ''\n",
        "\n",
        "\n",
        "    # Filter for rows that are not processed and have both StudyTitle and Disease\n",
        "    rows_to_process = input_df[\n",
        "        (input_df['Processed'] == '') &\n",
        "        (input_df['icd11_description'] != '') &\n",
        "        (input_df['icd11_description'] != 'None') &\n",
        "        (input_df['StudyTitle'] != '')\n",
        "    ].copy()\n",
        "\n",
        "    rows_to_process=rows_to_process[['DOI','StudyTitle','icd11_description']]\n",
        "\n",
        "    print(f\"Loaded {len(input_df)} total records.\")\n",
        "    print(f\"Found {len(rows_to_process)} new records to process.\")\n",
        "    print(\"--- Successfully loaded data from Google Sheet ---\")\n",
        "    display(DataTable(rows_to_process.head()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not load Google Sheet. Error: {e}\")\n",
        "    rows_to_process = pd.DataFrame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "UrhJls9Qs9hC",
        "outputId": "e7f3bfd5-6785-41f4-ff45-752714a54f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading data from Google Sheet ---\n",
            "Loaded 2329 total records.\n",
            "Found 836 new records to process.\n",
            "--- Successfully loaded data from Google Sheet ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOI</th>\n",
              "      <th>StudyTitle</th>\n",
              "      <th>icd11_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.3389/fcimb.2019.00476</td>\n",
              "      <td>The Oral Microbiota May Have Influence on Oral...</td>\n",
              "      <td>Malignant neoplasms of other or ill-defined si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.1038/s41467-024-53013-x</td>\n",
              "      <td>Effects of iron supplements and iron-containin...</td>\n",
              "      <td>Anaemias or other erythrocyte disorders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.1038/s41586-022-04427-4</td>\n",
              "      <td>The lung microbiome regulates brain autoimmunity</td>\n",
              "      <td>Multiple sclerosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.1128/spectrum.01901-21</td>\n",
              "      <td>Insights into the Unique Lung Microbiota Profi...</td>\n",
              "      <td>Multiple sclerosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10.1038/s41598-022-07995-7</td>\n",
              "      <td>16S rRNA and metagenomic shotgun sequencing da...</td>\n",
              "      <td>Ulcerative colitis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/9e65f7085e7ffcb7/data_table.js\";\n\n      const table = window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"10.3389/fcimb.2019.00476\",\n\"The Oral Microbiota May Have Influence on Oral Cancer\",\n\"Malignant neoplasms of other or ill-defined sites in the lip, oral cavity or pharynx\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"10.1038/s41467-024-53013-x\",\n\"Effects of iron supplements and iron-containing micronutrient powders on the gut microbiome in Bangladeshi infants: a randomized controlled trial\",\n\"Anaemias or other erythrocyte disorders\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"10.1038/s41586-022-04427-4\",\n\"The lung microbiome regulates brain autoimmunity\",\n\"Multiple sclerosis\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"10.1128/spectrum.01901-21\",\n\"Insights into the Unique Lung Microbiota Profile of Pulmonary Tuberculosis Patients Using Metagenomic Next-Generation Sequencing\",\n\"Multiple sclerosis\"],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"10.1038/s41598-022-07995-7\",\n\"16S rRNA and metagenomic shotgun sequencing data revealed consistent patterns of gut microbiome signature in pediatric ulcerative colitis\",\n\"Ulcerative colitis\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"DOI\"], [\"string\", \"StudyTitle\"], [\"string\", \"icd11_description\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n\n      function appendQuickchartButton(parentElement) {\n        let quickchartButtonContainerElement = document.createElement('div');\n        quickchartButtonContainerElement.innerHTML = `\n    <div id=\"df-4c5a1aea-6e1e-4bb4-a8c9-26fea88dff2d\">\n      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c5a1aea-6e1e-4bb4-a8c9-26fea88dff2d')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n      </button>\n\n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n      <script>\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() => {\n          let quickchartButtonEl =\n            document.querySelector('#df-4c5a1aea-6e1e-4bb4-a8c9-26fea88dff2d button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      </script>\n    </div>`;\n        parentElement.appendChild(quickchartButtonContainerElement);\n      }\n\n      appendQuickchartButton(table);\n    ",
            "text/plain": [
              "<google.colab.data_table.DataTable object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 5.3: Load the Microbe and Disease Dictionaries\n",
        "# ==============================================================================\n",
        "def load_and_prepare_dictionary(filepath, entity_type='microbe'):\n",
        "    \"\"\"Loads a hierarchical JSON and prepares it for entity recognition.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            structured_dict = json.load(f)\n",
        "\n",
        "        name_to_canonical_map = {}\n",
        "        for canonical, data in structured_dict.items():\n",
        "            name_to_canonical_map[canonical.lower()] = canonical # Map lowercase to canonical\n",
        "            for alias in data.get('aliases', []):\n",
        "                name_to_canonical_map[alias.lower()] = canonical\n",
        "\n",
        "        gazetteer = set(name_to_canonical_map.keys())\n",
        "\n",
        "        print(f\"âœ… {entity_type.capitalize()} dictionary loaded from '{filepath}'.\")\n",
        "        print(f\"   - {len(gazetteer)} total names/aliases mapped to {len(structured_dict)} canonical entities.\")\n",
        "        return name_to_canonical_map, gazetteer\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: The dictionary file '{filepath}' was not found.\")\n",
        "        return None, None\n",
        "\n",
        "# --- Load Microbe Dictionary ---\n",
        "microbe_map, microbe_gazetteer = load_and_prepare_dictionary(\n",
        "    microbe_dict, 'microbe'\n",
        ")\n",
        "\n",
        "# --- Load Disease Dictionary ---\n",
        "disease_map, disease_gazetteer = load_and_prepare_dictionary(\n",
        "    disease_dict, 'disease'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGfJXIDYwAa9",
        "outputId": "b90daeb6-925d-4337-a7c4-58d0afb0ccb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Microbe dictionary loaded from 'microbe_dictionary_hierarchical.json'.\n",
            "   - 571513 total names/aliases mapped to 512494 canonical entities.\n",
            "âœ… Disease dictionary loaded from 'disease_dictionary_hierarchical.json'.\n",
            "   - 71156 total names/aliases mapped to 37554 canonical entities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 5.4: Build and Load the Abstract Dictionary\n",
        "# ==============================================================================\n",
        "ABSTRACT_DICT_PATH = 'abstract_dictionary.json'\n",
        "\n",
        "# --- Build Dictionary (if it doesn't exist) ---\n",
        "fetcher = PublicationFetcher(email=EMAIL, api_key=NCBI_API_KEY)\n",
        "build_abstract_dictionary(fetcher, rows_to_process, ABSTRACT_DICT_PATH)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1MX99Kiw3bN",
        "outputId": "6e59d6d2-9b67-478b-aaaf-deca92280db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… PublicationFetcher initialized.\n",
            "\n",
            "ğŸ“š Building Abstract Dictionary... This may take a while.\n",
            "   - Processing DOI 269/822: 10.3390/ijms26115049  > WARNING: Request failed on attempt 1/3: 400 Client Error: Bad Request for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=40507861&retmode=xml&api_key=767eaa152eb483aacd290900fe1ddf9eb309&email=dmitry756%40gmail.com\n",
            "  > Retrying in 2 seconds...\n",
            "  > WARNING: Request failed on attempt 2/3: 400 Client Error: Bad Request for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=40507861&retmode=xml&api_key=767eaa152eb483aacd290900fe1ddf9eb309&email=dmitry756%40gmail.com\n",
            "  > Retrying in 4 seconds...\n",
            "   - Processing DOI 821/822: 10.1167/tvst.10.2.19\n",
            "âœ… Abstract fetching complete.\n",
            "ğŸ’¾ Abstract dictionary saved to 'abstract_dictionary.json'.\n",
            "âœ… Successfully loaded abstract dictionary with 821 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Dictionary ---\n",
        "try:\n",
        "    with open(ABSTRACT_DICT_PATH, 'r') as f:\n",
        "        abstract_dictionary = json.load(f)\n",
        "    print(f\"âœ… Successfully loaded abstract dictionary with {len(abstract_dictionary)} entries.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ERROR: Abstract dictionary not found. Please run the build step.\")\n",
        "    abstract_dictionary = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU6cRth4HruU",
        "outputId": "a339e3e1-a0f5-472e-b3ab-1f0ec74ae2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully loaded abstract dictionary with 821 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 6: Define the MicrobeExtractor Class\n",
        "# ==============================================================================\n",
        "\n",
        "class MicrobeExtractor:\n",
        "    \"\"\"\n",
        "    A highly optimized class to extract microbe names using pre-compiled,\n",
        "    batched regular expressions for maximum performance.\n",
        "    \"\"\"\n",
        "    def __init__(self, name_to_canonical_map, gazetteer, batch_size=500):\n",
        "        self.name_map = name_to_canonical_map\n",
        "        self.compiled_patterns = []\n",
        "\n",
        "        print(f\"âš™ï¸  Optimizing gazetteer for extraction...\")\n",
        "        # Sort by length descending to ensure longer names are matched first\n",
        "        sorted_gazetteer = sorted(list(gazetteer), key=len, reverse=True)\n",
        "\n",
        "        print(f\"   - Compiling {len(sorted_gazetteer)} aliases into regex batches of size {batch_size}...\")\n",
        "        # Chunk the gazetteer into batches\n",
        "        for i in range(0, len(sorted_gazetteer), batch_size):\n",
        "            batch = sorted_gazetteer[i:i + batch_size]\n",
        "            # Escape each alias to handle special regex characters safely\n",
        "            escaped_batch = [re.escape(alias) for alias in batch]\n",
        "            # Create a single large OR pattern for the batch\n",
        "            pattern_str = r'\\b(' + '|'.join(escaped_batch) + r')\\b'\n",
        "            # Compile the pattern for speed and add it to our list\n",
        "            self.compiled_patterns.append(re.compile(pattern_str, re.IGNORECASE))\n",
        "\n",
        "        print(f\"âœ… MicrobeExtractor initialized with {len(self.compiled_patterns)} compiled regex patterns.\")\n",
        "\n",
        "    def find_microbes(self, text):\n",
        "        \"\"\"Finds microbes using the pre-compiled batched regex patterns.\"\"\"\n",
        "        if not isinstance(text, str): return []\n",
        "\n",
        "        found_microbes = set()\n",
        "\n",
        "        # Iterate through the compiled regex patterns (batches)\n",
        "        for pattern in self.compiled_patterns:\n",
        "            # finditer finds all non-overlapping matches for the pattern\n",
        "            for match in pattern.finditer(text):\n",
        "                # The matched string\n",
        "                matched_alias = match.group(0)\n",
        "                # Normalize it to its canonical name\n",
        "                canonical_name = self.name_map.get(matched_alias.lower())\n",
        "                # The .get() is safer. Use .get(matched_alias.lower()) if your map keys are all lowercase\n",
        "                if canonical_name:\n",
        "                    found_microbes.add(canonical_name)\n",
        "\n",
        "        return list(found_microbes)"
      ],
      "metadata": {
        "id": "pInbpB1LwoHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 7: Corpus Creation with Context\n",
        "# ==============================================================================\n",
        "def preprocess_text_for_phrasing(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "    return [word for word in text.split() if word not in stop_words and len(word) > 2]\n",
        "\n",
        "if not rows_to_process.empty and microbe_map and disease_map:\n",
        "    microbe_extractor = MicrobeExtractor(microbe_map, microbe_gazetteer)\n",
        "    disease_extractor = MicrobeExtractor(disease_map, disease_gazetteer)\n",
        "\n",
        "    initial_corpus = []\n",
        "    associations = []\n",
        "    print(\"\\nScanning text and creating initial corpus...\")\n",
        "\n",
        "    for _, row in rows_to_process.iterrows():\n",
        "        doi = row['DOI']\n",
        "        title = row['StudyTitle']\n",
        "        canonical_disease = row['icd11_description']\n",
        "\n",
        "        # --- Get abstract from the pre-loaded dictionary ---\n",
        "        abstract = abstract_dictionary.get(doi, \"\")\n",
        "\n",
        "        # --- Combine title and abstract for a richer context ---\n",
        "        full_text = title\n",
        "        if abstract:\n",
        "            full_text += ' ' + abstract\n",
        "\n",
        "        canonical_microbes = microbe_extractor.find_microbes(full_text)\n",
        "        disease_aliases = disease_extractor.find_microbes(full_text)\n",
        "\n",
        "        if canonical_microbes and canonical_disease:\n",
        "            context_words = preprocess_text_for_phrasing(full_text)\n",
        "\n",
        "            training_sentence = canonical_microbes + context_words + [canonical_disease] + disease_aliases\n",
        "\n",
        "            if len(training_sentence) > 1:\n",
        "                initial_corpus.append(training_sentence)\n",
        "\n",
        "            for microbe in canonical_microbes:\n",
        "                associations.append({'microbe': microbe, 'disease': canonical_disease})\n",
        "\n",
        "    # --- Learn and apply multi-word phrases ---\n",
        "    print(\"ğŸ—£ï¸  Learning multi-word phrases\")\n",
        "    phrases = Phrases(initial_corpus, min_count=2, threshold=10.0)\n",
        "    phraser = Phraser(phrases)\n",
        "    # Apply the phraser to the whole corpus\n",
        "    final_corpus = [phraser[doc] for doc in initial_corpus]\n",
        "    print(\"âœ… Phrase detection complete.\")\n",
        "\n",
        "    print(f\"ğŸ¦  Found {len(associations)} microbe-disease associations.\")\n",
        "    # assoc_df = pd.DataFrame(associations).drop_duplicates()\n",
        "    # print(f\"ğŸ¦  Found {len(assoc_df)} unique microbe-disease associations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSd_OoiQfdOf",
        "outputId": "c4d92f85-8fca-48b8-cdfd-0660cd3e6f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸  Optimizing gazetteer for extraction...\n",
            "   - Compiling 571513 aliases into regex batches of size 500...\n",
            "âœ… MicrobeExtractor initialized with 1144 compiled regex patterns.\n",
            "âš™ï¸  Optimizing gazetteer for extraction...\n",
            "   - Compiling 71156 aliases into regex batches of size 500...\n",
            "âœ… MicrobeExtractor initialized with 143 compiled regex patterns.\n",
            "\n",
            "Scanning text and creating initial corpus...\n",
            "ğŸ—£ï¸  Learning multi-word phrases\n",
            "âœ… Phrase detection complete.\n",
            "ğŸ¦  Found 1277 microbe-disease associations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # --- Learn and apply multi-word phrases ---\n",
        "    print(\"ğŸ—£ï¸  Learning multi-word phrases from the richer corpus...\")\n",
        "    phrases = Phrases(initial_corpus, min_count=3, threshold=10.0)\n",
        "    phraser = Phraser(phrases)\n",
        "    final_corpus = [phraser[doc] for doc in initial_corpus]\n",
        "    print(\"âœ… Phrase detection complete.\")\n",
        "\n",
        "    # --- ğŸ“Š AGGREGATE CO-OCCURRENCE COUNTS ---\n",
        "    # Convert the full list of associations to a DataFrame\n",
        "    assoc_df_raw = pd.DataFrame(associations)\n",
        "\n",
        "    if not assoc_df_raw.empty:\n",
        "        # Group by microbe and disease, then count the size of each group\n",
        "        assoc_counts_df = assoc_df_raw.groupby(['microbe', 'disease']).size().reset_index(name='count')\n",
        "        assoc_counts_df = assoc_counts_df.sort_values(by='count', ascending=False)\n",
        "        print(f\"ğŸ“Š Found {len(assoc_counts_df)} unique microbe-disease associations with frequency counts.\")\n",
        "    else:\n",
        "        assoc_counts_df = pd.DataFrame(columns=['microbe', 'disease', 'count'])\n",
        "        print(\"âš ï¸ No associations found to count.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYvpOacOoIyo",
        "outputId": "8d83331b-9877-4838-844a-0d4435c7f686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—£ï¸  Learning multi-word phrases from the richer corpus...\n",
            "âœ… Phrase detection complete.\n",
            "ğŸ“Š Found 1033 unique microbe-disease associations with frequency counts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assoc_df_raw.to_csv('microbe_disease_associations_raw.csv', index=False)\n",
        "assoc_counts_df.to_csv('microbe_disease_associations_counts.csv', index=False)"
      ],
      "metadata": {
        "id": "WfeWlo7X6YmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8: Adjusting Model Hyperparameters for a Richer Corpus**\n",
        "\n",
        "The corpus, enriched with abstracts, is significantly larger and more contextually detailed than the one built only from titles. To get the most meaningful results from the Word2Vec model, adjust its hyperparameters:\n",
        "\n",
        "* **`window`**: The original value was `3`. With longer sentences from abstracts, we need to capture relationships between words that are further apart. **Increasing the window size to `5` or `7`** is a good starting point.\n",
        "* **`min_count`**: The original value was `2`. Our larger corpus will contain many more rare words (hapax legomena) that are essentially noise. **Increasing `min_count` to `3` or `5`** will help filter these out, leading to more robust and meaningful vectors for the remaining vocabulary.\n",
        "* **`epochs`**: The original value of `50` is quite high and excellent for a small corpus. With a larger corpus, the model sees more data per epoch. We can likely achieve good results with slightly fewer epochs (e.g., `20-30`), but keeping it at `50` will ensure thorough training if time permits."
      ],
      "metadata": {
        "id": "Uxvhj8B2zWQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 8: Train Word2Vec Model and Build the Final Graph\n",
        "# ==============================================================================\n",
        "if 'final_corpus' in locals() and final_corpus:\n",
        "    print(\"\\nğŸ§  Training Word2Vec model with tuned parameters...\")\n",
        "    # --- Tuned Hyperparameters ---\n",
        "    model = Word2Vec(\n",
        "        sentences=final_corpus,\n",
        "        vector_size=300,\n",
        "        window=5,      # Smaller window for more specific context\n",
        "        min_count=3,   # Ignore very rare words\n",
        "        workers=4,\n",
        "        sg=1,\n",
        "        epochs=50      # More training iterations on the small corpus\n",
        "    )\n",
        "    print(\"âœ¨ Model training complete!\")\n",
        "\n",
        "    G = nx.Graph()\n",
        "    print(f\"\\nğŸ•¸ï¸ Building graph from the found associations...\")\n",
        "    similarity_threshold = 0.5 # Adjust as needed, scores should be more meaningful now\n",
        "\n",
        "    for _, row in assoc_df.iterrows():\n",
        "        microbe = row['microbe']\n",
        "        disease = row['disease']\n",
        "        # The disease name needs to be phrased just like the training data\n",
        "        disease_phrase = '_'.join(preprocess_text_for_phrasing(disease))\n",
        "\n",
        "        try:\n",
        "            if microbe in model.wv and disease_phrase in model.wv:\n",
        "                score = float(model.wv.similarity(microbe, disease_phrase))\n",
        "                if score >= similarity_threshold:\n",
        "                    G.add_node(microbe, size=10, color='skyblue', title=f\"Microbe: {microbe}\")\n",
        "                    G.add_node(disease, size=20, color='tomato', title=f\"Disease: {disease}\")\n",
        "                    G.add_edge(microbe, disease, weight=score, title=f\"{score:.2f}\", label=f\"{score:.2f}\")\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    print(f\"   - Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges (Threshold > {similarity_threshold}).\")\n",
        "\n",
        "    if G.number_of_edges() > 0:\n",
        "        net = Network(notebook=True, cdn_resources='in_line', height='1600px', width='100%', bgcolor='#222222', font_color='white')\n",
        "        net.from_nx(G); net.show_buttons(filter_=['physics'])\n",
        "        net.save_graph(\"microbe_disease_graph_final_context.html\")\n",
        "        print(\"\\nğŸ‰ Interactive graph saved as 'microbe_disease_graph_final_context.html'.\")\n",
        "        display(net)\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ No relationships found. Try lowering the threshold or expanding the corpus with abstracts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "fWiXVN-IyOPj",
        "outputId": "ec4dd4a2-753a-4f94-c0ea-75d8240c086d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§  Training Word2Vec model with tuned parameters...\n",
            "âœ¨ Model training complete!\n",
            "\n",
            "ğŸ•¸ï¸ Building graph from the found associations...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'assoc_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-507005923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msimilarity_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;31m# Adjust as needed, scores should be more meaningful now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massoc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmicrobe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'microbe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdisease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'disease'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'assoc_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 8: Train Word2Vec Model and Build the Final Graph\n",
        "# ==============================================================================\n",
        "if 'final_corpus' in locals() and final_corpus:\n",
        "    print(\"\\nğŸ§  Training Word2Vec model with tuned parameters...\")\n",
        "    # --- Tuned Hyperparameters ---\n",
        "    model = Word2Vec(\n",
        "        sentences=final_corpus,\n",
        "        vector_size=300,\n",
        "        window=5,      # Smaller window for more specific context\n",
        "        min_count=3,   # Ignore very rare words\n",
        "        workers=4,\n",
        "        sg=1,\n",
        "        epochs=50      # More training iterations on the small corpus\n",
        "    )\n",
        "    print(\"âœ¨ Model training complete!\")\n",
        "\n",
        "G = nx.Graph()\n",
        "print(f\"\\nğŸ•¸ï¸ Building graph with a composite score for edge weights...\")\n",
        "similarity_threshold = 0.4\n",
        "# This new weight will now control the edge thickness and physics\n",
        "for _, row in assoc_counts_df.iterrows():\n",
        "    microbe = row['microbe']\n",
        "    disease = row['disease']\n",
        "    count = row['count']\n",
        "    disease_phrase = '_'.join(preprocess_text_for_phrasing(disease))\n",
        "\n",
        "    try:\n",
        "        if microbe in model.wv and disease_phrase in model.wv:\n",
        "            similarity_score = float(model.wv.similarity(microbe, disease_phrase))\n",
        "\n",
        "            # --- âœ… Calculate the new composite score ---\n",
        "            # We use np.log1p which is equivalent to log(1 + count)\n",
        "            composite_score = similarity_score * np.log1p(count)\n",
        "            if composite_score >= similarity_threshold:\n",
        "              # Add nodes as before\n",
        "              G.add_node(microbe, size=10, color='skyblue', title=f\"Microbe: {microbe}\")\n",
        "              G.add_node(disease, size=20, color='tomato', title=f\"Disease: {disease}\")\n",
        "\n",
        "              # Add the edge using the new composite score for weight\n",
        "              G.add_edge(\n",
        "                  microbe,\n",
        "                  disease,\n",
        "                  weight=composite_score, # Use the new score for physics\n",
        "                  title=f\"Composite Score: {composite_score:.2f}\\nSimilarity: {similarity_score:.2f}\\nCo-occurrences: {count}\",\n",
        "                  label=f\"{composite_score:.2f}\"\n",
        "              )\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "print(f\"   - Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "\n",
        "# --- The rest of the visualization code remains the same ---\n",
        "if G.number_of_edges() > 0:\n",
        "    net = Network(notebook=True, cdn_resources='in_line', height='1600px', width='100%', bgcolor='#222222', font_color='white')\n",
        "    # This tells Pyvis to make edge thickness dependent on the 'weight' attribute\n",
        "    net.from_nx(G)\n",
        "    net.show_buttons(filter_=['physics', 'edges'])\n",
        "    net.save_graph(\"microbe_disease_graph_composite_score.html\")\n",
        "    print(\"\\nğŸ‰ Interactive graph saved as 'microbe_disease_graph_composite_score.html'.\")\n",
        "    display(net)\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ No relationships found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "GAU-5AdHozwR",
        "outputId": "6615c4b3-da6e-451a-fe3a-f8d6ec5a16c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§  Training Word2Vec model with tuned parameters...\n",
            "âœ¨ Model training complete!\n",
            "\n",
            "ğŸ•¸ï¸ Building graph with a composite score for edge weights...\n",
            "   - Graph built with 41 nodes and 27 edges.\n",
            "\n",
            "ğŸ‰ Interactive graph saved as 'microbe_disease_graph_composite_score.html'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<class 'pyvis.network.Network'> |N|=41 |E|=27"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 8: Word2Vec Model\n",
        "# ==============================================================================\n",
        "if 'final_corpus' in locals() and final_corpus:\n",
        "    print(\"\\nğŸ§  Training Word2Vec model with tuned parameters...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=final_corpus,\n",
        "        vector_size=300,\n",
        "        window=7,\n",
        "        min_count=3,\n",
        "        workers=4,\n",
        "        sg=1,\n",
        "        epochs=50\n",
        "    )\n",
        "    print(\"âœ¨ Model training complete!\")\n",
        "\n",
        "    G = nx.Graph()\n",
        "    print(f\"\\nğŸ•¸ï¸ Building graph with composite score for physics and co-occurrence count for edge thickness...\")\n",
        "\n",
        "    for _, row in assoc_counts_df.iterrows():\n",
        "        microbe = row['microbe']\n",
        "        disease = row['disease']\n",
        "        count = row['count']\n",
        "        disease_phrase = '_'.join(preprocess_text_for_phrasing(disease))\n",
        "\n",
        "        similarity_threshold = 0.3\n",
        "\n",
        "        try:\n",
        "            if microbe in model.wv and disease_phrase in model.wv:\n",
        "                similarity_score = float(model.wv.similarity(microbe, disease_phrase))\n",
        "                composite_score = similarity_score * np.log1p(count)\n",
        "                if composite_score >= similarity_threshold:\n",
        "                  # --- âœ… Calculate log-scaled count for thickness ---\n",
        "                  # np.log1p(count) is equivalent to np.log(count + 1)\n",
        "                  edge_thickness = np.log1p(count)\n",
        "\n",
        "                  # Add nodes as before\n",
        "                  G.add_node(microbe, size=10, color='skyblue', title=f\"Microbe: {microbe}\")\n",
        "                  G.add_node(disease, size=20, color='tomato', title=f\"Disease: {disease}\")\n",
        "\n",
        "                  # --- âœ… Add 'value' attribute to control thickness ---\n",
        "                  G.add_edge(\n",
        "                      microbe,\n",
        "                      disease,\n",
        "                      weight=composite_score, # Use the composite score for physics\n",
        "                      value=edge_thickness, # Use the log-scaled count for visual thickness\n",
        "                      label=f\"{composite_score:.2f}\",\n",
        "                      title=f\"Composite Score: {composite_score:.2f}\\nSimilarity: {similarity_score:.2f}\\nCo-occurrences: {count}\"\n",
        "                )\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    print(f\"   - Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "    print(f\"   - Edge thickness now represents the log-scaled co-occurrence count.\")\n",
        "\n",
        "    if G.number_of_edges() > 0:\n",
        "        net = Network(notebook=True, cdn_resources='in_line', height='1600px', width='100%', bgcolor='#222222', font_color='white')\n",
        "        net.from_nx(G)\n",
        "        net.show_buttons(filter_=['physics', 'edges'])\n",
        "        net.save_graph(\"microbe_disease_graph_thickness.html\")\n",
        "        print(\"\\nğŸ‰ Interactive graph saved as 'microbe_disease_graph_thickness.html'.\")\n",
        "        display(net)\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ No relationships found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "lBbKazyRqh1A",
        "outputId": "8eb85a39-4663-4806-dfec-8d79d8ec636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§  Training Word2Vec model with tuned parameters...\n",
            "âœ¨ Model training complete!\n",
            "\n",
            "ğŸ•¸ï¸ Building graph with composite score for physics and co-occurrence count for edge thickness...\n",
            "   - Graph built with 54 nodes and 49 edges.\n",
            "   - Edge thickness now represents the log-scaled co-occurrence count.\n",
            "\n",
            "ğŸ‰ Interactive graph saved as 'microbe_disease_graph_thickness.html'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<class 'pyvis.network.Network'> |N|=54 |E|=49"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Next Steps and Scaling Up\n",
        "\n",
        "###Expand the Corpus:\n",
        "\n",
        "* âœ… Fetch PubMed Abstracts: Modify the script to use the requests to get PubMed abstracts for each DOI in your sheet.\n",
        "\n",
        "###Improve Entity Recognition:\n",
        "\n",
        "* Create Dictionaries:\n",
        "  * âœ… Compile comprehensive lists of microbe names (at species and genus levels) and disease synonyms from resources like the NCBI Taxonomy.\n",
        "  * âœ… Create comprehensive disease dictionary from  ICD-11 or similar taxonomy\n",
        "\n",
        "* Use BioBERT: FUse a pre-trained language model like BioBERT for Named Entity Recognition.\n",
        "\n",
        "###Refine the Graph:\n",
        "\n",
        "* Node Attributes: Add more metadata to nodes. For a disease node, add its \"Disease Group.\" For a microbe, add its phylum. This can be used for more advanced filtering and coloring in visualization.\n",
        "\n",
        "* âœ… Edge Weighting: Experiment with different similarity thresholds and consider weighting edges by how many times two entities co-occur in corpus.\n",
        "\n",
        "###Advanced Analysis:\n",
        "\n",
        "* Community Detection: Use algorithms like the Louvain method in NetworkX to find \"communities\" or clusters of tightly connected nodes. This could reveal, a group of different bacteria all associated with metabolic disorders.\n",
        "\n",
        "* Link Prediction: Use graph machine learning techniques to predict missing links, suggesting novel microbe-disease relationships that are plausible but not yet explicitly studied."
      ],
      "metadata": {
        "id": "FuER-2jRqpdQ"
      }
    }
  ]
}